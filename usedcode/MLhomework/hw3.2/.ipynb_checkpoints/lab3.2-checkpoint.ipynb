{"metadata":{"language_info":{"name":"python","version":"3.8.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 内容提要\n- 原理与`sklearn`实现\n  - 类间隔优化及其对偶问题\n  - 间隔的软化\n  - 在`sklearn`中调用支持向量机算法\n- 核方法的引入\n  - 常见的非线性核\n- 超参数优化\n  - 回顾: 标准流程\n  - 用`sklearn.model_selection`内置工具管理超参数优化","metadata":{},"id":"130fab27-218d-4166-9661-ca76f6c1567a"},{"cell_type":"code","source":"!pip install -U scikit-learn","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (1.0.2)\nCollecting scikit-learn\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3f/48/6fdd99f5717045f9984616b5c2ec683d6286d30c0ac234563062132b83ab/scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.17.3 in /opt/conda/lib/python3.8/site-packages (from scikit-learn) (1.22.4)\nRequirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn) (1.7.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\nInstalling collected packages: scikit-learn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.0.2\n    Uninstalling scikit-learn-1.0.2:\n      Successfully uninstalled scikit-learn-1.0.2\nSuccessfully installed scikit-learn-1.3.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"id":"e4819ef0-625a-4c38-9c8d-e50e637e4109"},{"cell_type":"markdown","source":"# 原理与`sklearn`实现\n\n$\\quad$我们考虑一个(参数化的、非概率的)二元分类器\n\n$$\n\\tilde{y} = \\mathrm{sgn}(f(\\mathbf{x};\\mathbf{w})) \\equiv \\mathrm{sgn}(\\mathbf{w}^\\mathrm{T}\\mathbf{x} + w_0),\n$$\n\n用于预测类别标签$\\tilde{t} \\in \\{-1, 1\\}$. 其中, 超平面$0 = f(\\mathbf{x};\\mathbf{w})$显然成为了决策边界.\n\n$\\quad$我们先在本节假定(assume)训练数据$\\mathcal{D} = \\{(\\mathbf{x}^{(n)}, \\tilde{t}^{(n)})\\}_{n=1}^N$是**线性可分的**(linearly separable), 并试图在(无穷多个)成功分割了训练数据的决策平面中选择**稳健性**(robustness)最强的那个. 为此, **支持向量机**(support vector machine, SVM)算法将选择这样的超平面: *使得给定类别的样本点到该平面的最小距离仍然不小*, 也就是实现**最短距离的最大化**.\n\n![alt image.png](https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17993/bc01554bbe7d4c05a1fae15a7bfdd714/yIkQ32rGFqnUz6gNkZV2ww.png)\n\n在上图中, 对给定的样本点$\\mathbf{x}$, 若以$r$代指有向距离, 则我们将单位法向量$\\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|}$倍乘$r$并从$\\mathbf{x}$中扣除, 将得到$\\mathbf{x}$在超平面$0=f$上的正交投影\n\n$$\n\\mathbf{x}_\\perp = \\mathbf{x} - r\\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|},\n$$\n\n进而\n$$\\begin{aligned}\n0\n&= f(\\mathbf{x}_\\perp)\\\\\n&= \\mathbf{w}^\\mathrm{T}\\left(\\mathbf{x} - r\\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|}\\right) + w_0\\\\\n&= f(\\mathbf{x}) - r\\|\\mathbf{w}\\|,\n\\end{aligned}$$\n\n于是, 解得\n\n$$\nr = \\frac{f(\\mathbf{x})}{\\|\\mathbf{w}\\|}.\n$$\n\n$\\quad$不难验证, 对于线性可分的数据集$\\mathcal{D}$, $r$总是与类别标签$\\tilde{t}$同号. 于是, 对样本$n$, 待考察的优化目标(类间隔)$|r|$可写为\n\n$$\n\\frac{\\tilde{t}^{(n)}f(\\mathbf{x}^{(n)})}{\\|\\mathbf{w}\\|} = \\frac{\\tilde{t}^{(n)}(\\mathbf{w}^\\mathrm{T}\\mathbf{x}^{(n)} + w_0)}{\\|\\mathbf{w}\\|}.\n$$","metadata":{},"id":"dd374eea-5b67-47b0-8d4e-98661f611084"},{"cell_type":"markdown","source":"## 类间隔优化及其对偶问题\n\n$\\quad$我们试图寻找最优参数$(\\mathbf{w}^*, w_0^*)$, 使之最大化$\\mathcal{D}$上的最小类间隔. 于是,\n\n$$\n(\\mathbf{w}^*, w_0^*) = \\arg{\\max_{(\\mathbf{w}, w_0)}{\\frac{1}{\\|\\mathbf{w}\\|}\\min_{n=1}^N{\\tilde{t}^{(n)}(\\mathbf{w}^\\mathrm{T}\\mathbf{x}^{(n)} + w_0)}}}.\n$$\n\n但注意$\\mathbf{w}, w_0$的倍乘不影响分类结果, 于是, 不妨采用**规范表示**(canonical representation), 取$1=\\min_{n=1}^N{\\tilde{t}^{(n)}(\\mathbf{w}^\\mathrm{T}\\mathbf{x}^{(n)} + w_0)}$, 上式化为**约束优化**(constrained optimization)问题\n\n$$\n(\\mathbf{w}^*, w_0^*) = \\arg{\\max_{(\\mathbf{w}, w_0)}{\\frac{1}{\\|\\mathbf{w}\\|}}},\\,\\,\\mathrm{s.t.}\\,\\,\\tilde{t}^{(n)}(\\mathbf{w}^\\mathrm{T}\\mathbf{x}^{(n)} + w_0) \\ge 1, n = 1, \\dots, N.\n$$\n\n- **原问题**(primal problem): 上式等价地写为凸优化问题\n  $$\n  (\\mathbf{w}^*, w_0^*) = \\arg{\\min_{(\\mathbf{w}, w_0)}{\\frac{1}{2}\\|\\mathbf{w}\\|^2}},\\,\\,\\mathrm{s.t.}\\,\\,\\tilde{t}^{(n)} (\\mathbf{w}^\\mathrm{T}\\mathbf{x}^{(n)} + w_0) \\ge 1, n = 1, \\dots, N,\n  $$\n  这是一个**二次规划**(quadratic programming, QP)问题, 存在标准化的算法用于求解.\n- **对偶问题**(dual problem): 引入(非负的)Lagrange乘子$\\boldsymbol{\\lambda} \\in \\mathbb{R}_+^N$, 并等价地将原问题写为\n  $$\\begin{aligned}\n  (\\mathbf{w}^*, w_0^*, \\boldsymbol{\\lambda}^*) &= \\arg{\\min_{(\\mathbf{w}, w_0)}{\\max_{\\boldsymbol{\\lambda} \\ge \\mathbf{0}}{\\mathcal{L}}}},\\\\\n  \\mathcal{L} &= \\frac{1}{2}\\|\\mathbf{w}\\|^2 - \\sum_{n=1}^N{\\lambda_n\\left(\\tilde{t}^{(n)}(\\mathbf{w}^\\mathrm{T}\\mathbf{x}^{(n)} + w_0) - 1\\right)},\n  \\end{aligned}$$\n  此时, 可以证明, 在[Karush-Kuhn-Tucker(KKT)条件](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions)下, 原问题的最优解也将是下述**对偶问题**(变换了“最小”与“最大”的求解顺序)的最优解:\n  $$\n  (\\mathbf{w}^*, w_0^*, \\boldsymbol{\\lambda}^*) = \\arg{\\max_{\\boldsymbol{\\lambda} \\ge \\mathbf{0}}{\\min_{(\\mathbf{w}, w_0)}{\\mathcal{L}}}}.\n  $$\n  - 内层的优化问题可以解析地求解:\n    $$\\begin{aligned}\n    \\mathbf{0} &= \\nabla_\\mathbf{w}\\mathcal{L} = \\mathbf{w} - \\sum_{n=1}^N{\\lambda_n\\tilde{t}^{(n)}\\mathbf{x}^{(n)}},\\\\\n    0 &= \\frac{\\partial}{\\partial w_0}\\mathcal{L} = -\\sum_{n=1}^N{\\lambda_n\\tilde{t}^{(n)}},\n    \\end{aligned}$$\n    截距项$w_0^*$则由决策边界$1 = \\mathbf{w}^\\mathrm{T}\\mathbf{x}^{(l)} + w_0$求解得到, $l$代指的样本为规范表示中的“边界”.\n  - 将这组解代入外层优化问题:\n    $$\\begin{aligned}\n    \\boldsymbol{\\lambda}^* &= \\arg{\\max_{\\boldsymbol{\\lambda} \\ge \\mathbf{0}}{\\left\\{\n    \\frac{1}{2}\\left\\|\\sum_{n=1}^N{\\lambda_n\\tilde{t}^{(n)}\\mathbf{x}^{(n)}}\\right\\|^2 - \\sum_{n=1}^N{\\lambda_n\\left(\n    \\tilde{t}^{(n)}\\left(\\left(\\sum_{m=1}^N{\\lambda_m\\tilde{t}^{(m)}\\mathbf{x}^{(m)}}\\right)^\\mathrm{T}\\mathbf{x}^{(n)} + w_0\n    \\right) - 1\\right)}\\right\\}}}\\\\\n    &= \\arg{\\max_{\\boldsymbol{\\lambda} \\ge \\mathbf{0}}{\\left\\{\n    \\sum_{n=1}^N{\\lambda_n} - \\frac{1}{2}\\sum_{m=1}^N{\\sum_{n=1}^N{\n    \\lambda_m\\lambda_n\\tilde{t}^{(m)}\\tilde{t}^{(n)}\\left<\\mathbf{x}^{(m)}, \\mathbf{x}^{(n)}\\right>\n    }}\\right\\}}},\n    \\end{aligned}$$\n    式中, $\\left<\\mathbf{x}, \\mathbf{y}\\right> \\equiv \\mathbf{x}^\\mathrm{T}\\mathbf{y}$定义了**内积**(inner product)运算, 本质上是某种“相似性”的度量. 支持向量机作为**核方法**(kernel method)的一种, 核心原理是试图基于样本的相似性来学习类别归属、做出分类推断.\n  - 可以用标准化的QP或更高效的[**序列最小优化**(sequential minimal optimization, SMO)](https://en.wikipedia.org/wiki/Sequential_minimal_optimization)算法求解. 同时, 以内积的形式描述特征$\\mathbf{x}$之间的关系, 这对后续引入核方法是颇为有益的.","metadata":{},"id":"1760d07c-b4b9-4008-984b-40c2f94f9358"},{"cell_type":"markdown","source":"## 间隔的软化\n\n$\\quad$面对线性不可分的数据, 我们适度允许分类误差, 以缓解过拟合现象、提高模型(在个别离群点上)的稳健性. 此时, 引入(非负)松弛变量$\\{\\xi_n\\}_{n=1}^N \\in \\mathbb{R}_+^N$, 并将约束条件放宽为\n\n$$\n\\tilde{t}^{(n)}(\\mathbf{w}^\\mathrm{T}\\mathbf{x}^{(n)} + w_0) \\ge 1 - \\xi_n, n = 1, \\dots, N.\n$$\n\n此时, 若将$\\{\\xi_n\\}$看作一组独立变量, 并引入软度超参数$C$, 则原问题与对偶问题将重新写为:\n\n- 原问题:\n  $$\n  (\\mathbf{w}^*, w_0^*) = \\arg{\\min_{(\\mathbf{w}, w_0)}{\\left\\{\\frac{1}{2}\\|\\mathbf{w}\\|^2 + C\\sum_{n=1}^N{\\xi_n}\\right\\}}},\\,\\,\\mathrm{s.t.}\\,\\,\\xi_n \\ge 0, \\tilde{t}^{(n)} (\\mathbf{w}^\\mathrm{T}\\mathbf{x}^{(n)} + w_0) \\ge 1 - \\xi_n, n = 1, \\dots, N;\n  $$\n- 对偶问题:\n  $$\n  \\boldsymbol{\\lambda}^* = \\arg{\\max_{\\boldsymbol{\\lambda} \\ge \\mathbf{0}}{\\left\\{\n    \\sum_{n=1}^N{\\lambda_n} - \\frac{1}{2}\\sum_{m=1}^N{\\sum_{n=1}^N{\n    \\lambda_m\\lambda_n\\tilde{t}^{(m)}\\tilde{t}^{(n)}\\left<\\mathbf{x}^{(m)}, \\mathbf{x}^{(n)}\\right>\n    }}\\right\\}}},\\,\\,\\mathrm{s.t.}\\,\\, 0 \\le \\lambda_n \\le C (n = 1, \\dots, N), 0 = \\sum_{n=1}^N{\\lambda_n\\tilde{t}^{(n)}}.\n  $$\n- $C$值越小, 模型的间隔越“软”、对个别点的错分(misclassification)越宽容, 欠拟合的倾向性也就越强. $C=\\infty$对应于前述的硬间隔(hard margin)问题.","metadata":{},"id":"d4a8b6f5-4976-4a20-af58-0a6f5d96fc9e"},{"cell_type":"markdown","source":"## 在`sklearn`中调用支持向量机算法\n\n$\\quad$支持向量机分类器封装于[`sklearn.svm.SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn-svm-svc), 创建模型后, 仍然是以[`fit()`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.fit)方法进行模型训练、[`predict()`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.predict)方法进行推断(预测).\n- **训练**阶段, 模型将数值地求解原问题或对偶问题(不同的算法模块有不同的具体实现), 寻找最优参数$\\boldsymbol{\\lambda}^*$.\n- **预测**阶段, 模型对给定样本计算决策函数\n  $$\\begin{aligned}\n  f(\\mathbf{x}; \\mathbf{w}) &= \\mathbf{w}^\\mathrm{T}(\\mathbf{x} - \\mathbf{x}^{(l)}) + 1\\\\\n  &= \\left(\\sum_{n=1}^N{\\lambda_n\\tilde{t}^{(n)}\\mathbf{x}^{(n)}}\\right)^\\mathrm{T} (\\mathbf{x} - \\mathbf{x}^{(l)}) + 1,\n  \\end{aligned}$$\n  并以$\\mathrm{sgn}(f(\\mathbf{x};\\mathbf{w}))$值给出分类决策.\n  - 可以证明, SVM总满足**KKT条件**. 此时, 对每个样本$n$, Lagrange参数和间隔值二者必有一个为零, 即:\n    $$\n    0 = \\boldsymbol{\\lambda} \\odot \\left(1 - \\tilde{t}^{(n)}\\tilde{y}^{(n)}\\right).\n    $$\n    决策阶段真正重要的是那些$\\lambda \\ne 0$的样本, 它们称为**支持向量**(support vector), 是“最危险的”样本, 间隔取最小值.\n\n$\\quad$我们导入一组二维的[人造数据集](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html#plot-classification-boundaries-with-different-svm-kernels)用于演示.","metadata":{},"id":"9799b642-2e7a-4c4b-a64f-6c045976c8b3"},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nX = np.array([\n        [0.4, -0.7],\n        [-1.5, -1.0],\n        [-1.4, -0.9],\n        [-1.3, -1.2],\n        [-1.1, -0.2],\n        [-1.2, -0.4],\n        [-0.5, 1.2],\n        [-1.5, 2.1],\n        [1.0, 1.0],\n        [1.3, 0.8],\n        [1.2, 0.5],\n        [0.2, -2.0],\n        [0.5, -2.4],\n        [0.2, -2.3],\n        [0.0, -2.7],\n        [1.3, 2.1],\n])\n\ny = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n\n# Plotting settings\nfig, ax = plt.subplots(figsize=(4, 3))\nx_min, x_max, y_min, y_max = -3, 3, -3, 3\nax.set(xlim=(x_min, x_max), ylim=(y_min, y_max))\n\n# Plot samples by color and add legend\nscatter = ax.scatter(X[:, 0], X[:, 1], s=150, c=y, label=y, edgecolors=\"k\")\nax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Classes\")\nax.set_title(\"Samples in two-dimensional feature space\")\nplt.show()","metadata":{"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 400x300 with 1 Axes>","remote/url":"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17993/dec2942d5e5940109483ea5a028623c0/eXwH1IwmV4f9w3KyQKBa0w.png"},"metadata":{}}],"id":"b370ffa4-b35d-43a6-98d4-0efad620ff9d"},{"cell_type":"markdown","source":"$\\quad$以下是一个辅助函数, 利用[`sklearn.inspection`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.inspection)模块的可视化工具[`DecisionBoundaryDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html#sklearn-inspection-decisionboundarydisplay)帮助我们可视化模型的**决策边界**(decision boundary). 我们先使用最简单的线性SVM分类器, 参数设置为`kernel=\"linear\"`.","metadata":{},"id":"4146c399-04e6-49be-942c-c1991a006fcf"},{"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\ndef plot_training_data_with_decision_boundary(kernel):\n    # Train the SVC\n    clf = svm.SVC(kernel=kernel, gamma=2).fit(X, y)\n\n    # Settings for plotting\n    _, ax = plt.subplots(figsize=(4, 3))\n    x_min, x_max, y_min, y_max = -3, 3, -3, 3\n    ax.set(xlim=(x_min, x_max), ylim=(y_min, y_max))\n\n    # Plot decision boundary and margins\n    common_params = {\"estimator\": clf, \"X\": X, \"ax\": ax}\n    DecisionBoundaryDisplay.from_estimator(\n        **common_params,\n        response_method=\"predict\",\n        plot_method=\"pcolormesh\",\n        alpha=0.3,\n    )\n    DecisionBoundaryDisplay.from_estimator(\n        **common_params,\n        response_method=\"decision_function\",\n        plot_method=\"contour\",\n        levels=[-1, 0, 1],\n        colors=[\"k\", \"k\", \"k\"],\n        linestyles=[\"--\", \"-\", \"--\"],\n    )\n\n    # Plot bigger circles around samples that serve as support vectors\n    ax.scatter(\n        clf.support_vectors_[:, 0],\n        clf.support_vectors_[:, 1],\n        s=250,\n        facecolors=\"none\",\n        edgecolors=\"k\",\n    )\n    # Plot samples by color and add legend\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=150, edgecolors=\"k\")\n    ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Classes\")\n    ax.set_title(f\"Decision boundaries of {kernel} kernel in SVC\")\n\n    plt.show()","metadata":{"trusted":true},"execution_count":3,"outputs":[],"id":"40eb5288-00ca-4669-8cf8-1e1f9081e65c"},{"cell_type":"markdown","source":"$\\quad$图中,\n- 用实线示出决策边界, 虚线示出间隔值.\n- 用空圈标明轮廓的样本是支持向量.","metadata":{},"id":"0d25b5fb-a9e2-4205-bd70-b269e13b3161"},{"cell_type":"code","source":"plot_training_data_with_decision_boundary(\"linear\")","metadata":{"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 400x300 with 1 Axes>","remote/url":"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17993/dec2942d5e5940109483ea5a028623c0/RlVIg4ZXS6M0UlvN-erbaA.png"},"metadata":{}}],"id":"9eabbb6b-5a6f-4377-a270-b369ed34339c"},{"cell_type":"markdown","source":"# 核方法的引入\n\n$\\quad$为了提高模型的表示能力(或者至少可以处理线性不可分的数据), 常对特征$\\mathbf{x} \\in \\mathcal{X}$作某种非线性变换$$\\phi: \\mathcal{X} \\to \\mathcal{M},\\mathbf{x}\\mapsto\\phi(\\mathbf{x}),$$使训练样本在空间$\\mathbf{M}$中具有更好的分布结构. 但比起研究$\\phi$的形式本身, 一个计算效率更高的做法是直接研究“内积”:$$K_\\phi: \\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}, K_\\phi(\\mathbf{x}, \\mathbf{x}') = \\left<\\phi(\\mathbf{x}), \\phi(\\mathbf{x}')\\right>_\\mathcal{M}.$$\n\n$\\quad$根据[Schölkopf-Herbrich-Smola表示定理](https://en.wikipedia.org/wiki/Representer_theorem), 只需要核函数$K_\\phi$是**正定的**(positive-definite), 则经过训练后的决策平面$f(\\mathbf{x})$总可以表示为有限项核函数的线性组合. 以下将介绍的几种常见核函数都是正定的. 核函数在设计上具有非常强的灵活性, 并且允许对特定数据引入特定领域的专业知识.\n\n$\\quad$定义**Gram矩阵**$$\\mathbf{K}_\\phi := \\left(\\left<\\phi(\\mathbf{x}^{(i)}), \\phi(\\mathbf{x}^{(j)})\\right>_\\mathcal{M}\\right)_{1 \\le i,j \\le N},$$则引入了核方法的SVM(kernel-SVM)将求解下述的对偶问题:\n$$\n\\boldsymbol{\\lambda}^* = \\arg{\\max_{\\boldsymbol{\\lambda} \\ge \\mathbf{0}}{\\left\\{\n    \\sum_{n=1}^N{\\lambda_n} - \\frac{1}{2}\\sum_{m=1}^N{\\sum_{n=1}^N{\n    \\lambda_m\\lambda_n\\tilde{t}^{(m)}\\tilde{t}^{(n)}(\\mathbf{K}_\\phi)_{m,n}\n    }}\\right\\}}},\\,\\,\\mathrm{s.t.}\\,\\, 0 \\le \\lambda_n \\le C (n = 1, \\dots, N), 0 = \\sum_{n=1}^N{\\lambda_n\\tilde{t}^{(n)}}.\n$$","metadata":{},"id":"fe553273-9f4a-4036-993c-50749845cdce"},{"cell_type":"markdown","source":"## 常见的非线性核","metadata":{},"id":"03985bee-9942-47c8-b15d-fdf586fe80d6"},{"cell_type":"markdown","source":"- 多项式核函数`kernel=\"poly\"`\n  $$\n  K(\\mathbf{x}, \\mathbf{x}') = (\\gamma\\mathbf{x}^\\mathrm{T}\\mathbf{x}' + r)^d.\n  $$\n  超参数包括:\n  - `gamma`, 对应内积前的系数$\\gamma$, 反映**对单个训练样本的敏感程度**. 它的默认取值为\n    $$\n    \\gamma_0 = \\frac{1}{N\\sigma^2(\\mathbf{X})},\n    $$\n    其中, $N$为训练样本数, $\\sigma^2(\\mathbf{X})$为数据方差.\n  - `coef0`, 对应截距项$r$, 默认值为0;\n  - `degree`, 对应多项式的度数$d$, 调节**模型的复杂程度**, 默认值为3.\n- 决策边界与决策间隔会随着训练数据的分布而相应适当“弯曲”.","metadata":{},"id":"77d9ba2b-aa3f-4b11-806f-35fe66674000"},{"cell_type":"code","source":"plot_training_data_with_decision_boundary(\"poly\")","metadata":{"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 400x300 with 1 Axes>","remote/url":"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17993/dec2942d5e5940109483ea5a028623c0/GK5lX5A7l6mdIyodBG3Chw.png"},"metadata":{}}],"id":"b397a2b4-8f62-4093-8b4c-74440b8fe5fd"},{"cell_type":"markdown","source":"- Sigmoid核函数`kernel=\"sigmoid\"`\n  $$\n  K(\\mathbf{x}, \\mathbf{x}') = \\tanh{(\\gamma\\mathbf{x}^\\mathrm{T}\\mathbf{x}' + r)},\n  $$\n  - 超参数包括`gamma`和`coef0`, 分别代指$\\gamma$与$r$.\n- 实践中往往不推荐, 因为决策边界通常过于复杂且难以泛化(除非数据有着特殊的分布结构).","metadata":{},"id":"7bba9d1c-652b-43da-b518-7ce03834e344"},{"cell_type":"code","source":"plot_training_data_with_decision_boundary(\"sigmoid\")","metadata":{"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 400x300 with 1 Axes>","remote/url":"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17993/dec2942d5e5940109483ea5a028623c0/ED_c2R649G_CSUITVwLejA.png"},"metadata":{}}],"id":"b6459143-137d-4a41-83d4-5a43260189c5"},{"cell_type":"markdown","source":"- Gaussian核函数`kernel=\"rbf\"`\n  $$\n  K(\\mathbf{x}, \\mathbf{x}') = \\exp{\\left(-\\gamma\\|\\mathbf{x} - \\mathbf{x}'\\|^2\\right)},\n  $$\n  - 超参数为`gamma`.\n- 对应的非线性表示$\\phi$有无穷维, 以Euclidean距离度量样本相似性, 因此决策边界也重在聚集距离相近的若干样本点. **它是`SVC`算法默认的核函数**.","metadata":{},"id":"72e4d848-6792-4b98-91ed-fb436873680d"},{"cell_type":"code","source":"plot_training_data_with_decision_boundary(\"rbf\")","metadata":{"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 400x300 with 1 Axes>","remote/url":"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17993/dec2942d5e5940109483ea5a028623c0/am1FavjMSo2b5LRjQTYhaw.png"},"metadata":{}}],"id":"81139316-70ab-4c6c-9f77-924ebbe830ad"},{"cell_type":"markdown","source":"# 超参数优化\n\n![alt image.png](https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17993/a7014363f4954f5c8efc6b934e595800/85yhZBaNYLyWKp_0i-m68Q.png)\n\n用伪代码表示如下.\n```\nmetrics = []\nfor config in search_space:\n    metrics.append(cross_validation(train_val_dataset, config))\nbest_config = argmax(metrics)\nfinal_model = train(train_val_dataset, best_config)\n```","metadata":{},"id":"3eaae83c-f663-402f-ad81-0b65a14e7cd3"},{"cell_type":"markdown","source":"## 用`sklearn.model_selection`内置工具管理超参数优化\n\n$\\quad$在[`sklearn.model_selection`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)模块中, 封装了用于超参数优化的[若干方法](https://scikit-learn.org/stable/modules/classes.html#hyper-parameter-optimizers), 可以一次性实现超参数的搜索空间采样与交叉验证. 每个优化方法都仍然是一个`sklearn`模型对象, 以`fit()`方法完成交叉验证. 在创建对象时, 传入的控制参数包括:\n- `estimator`: 用于超参数优化的模型;\n- `param_grid`: 超参数的**搜索空间**(search space), 定义成一个字典;\n- `scoring`: 评估指标函数, 可以是[特定字符串](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values)或一个(返回标量值的)函数, 或多个字符串/函数; 默认为`None`, 采取“默认”指标(例如分类问题是accuracy).\n- `cv`: 交叉验证的方式, 可以是整数(交叉验证的折数)、CV分割对象、迭代器, 等等; 默认使用5-折交叉验证.\n\n其中, 前两个参数是必需的.\n\n$\\quad$我们下面演示[`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn-model-selection-gridsearchcv)方法, 即实现**网格搜索**: 传入的`param_grid`中每个超参数都对应到一个**取值列表**, 网格搜索法将穷举所有的超参数组合并逐个完成交叉验证. 随后,\n- 可用`cv_results_`属性访问各个超参数组合在交叉验证中的表现结果;\n- 可以直接调用`predict()`等方法进行模型预测, `sklearn`将自动调用最优模型.","metadata":{},"id":"55e2ef61-eb03-4f58-bf86-8dae23cdcf09"},{"cell_type":"code","source":"from sklearn import svm, datasets\nfrom sklearn.model_selection import GridSearchCV\n\nestimator = svm.SVC()\nparam_grid = {\"C\": [1, 10], \"gamma\": [0.1, 0.2]}\n\ncv = GridSearchCV(estimator, param_grid, n_jobs=-1, scoring=\"accuracy\")\ncv.fit(X, y)","metadata":{"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"GridSearchCV(estimator=SVC(), n_jobs=-1,\n             param_grid={'C': [1, 10], 'gamma': [0.1, 0.2]},\n             scoring='accuracy')","text/html":"<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=SVC(), n_jobs=-1,\n             param_grid={&#x27;C&#x27;: [1, 10], &#x27;gamma&#x27;: [0.1, 0.2]},\n             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=SVC(), n_jobs=-1,\n             param_grid={&#x27;C&#x27;: [1, 10], &#x27;gamma&#x27;: [0.1, 0.2]},\n             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}],"id":"69853455-f55e-451e-bf18-f63eebdc8946"},{"cell_type":"code","source":"cv.cv_results_","metadata":{"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'mean_fit_time': array([0.00155249, 0.00128312, 0.00134087, 0.00119324]),\n 'std_fit_time': array([2.16112501e-04, 7.93720337e-05, 1.14200956e-04, 9.61521054e-05]),\n 'mean_score_time': array([0.00120606, 0.00105948, 0.00104637, 0.00110822]),\n 'std_score_time': array([1.85870831e-04, 1.33210927e-05, 2.99100455e-05, 6.48276558e-05]),\n 'param_C': masked_array(data=[1, 1, 10, 10],\n              mask=[False, False, False, False],\n        fill_value='?',\n             dtype=object),\n 'param_gamma': masked_array(data=[0.1, 0.2, 0.1, 0.2],\n              mask=[False, False, False, False],\n        fill_value='?',\n             dtype=object),\n 'params': [{'C': 1, 'gamma': 0.1},\n  {'C': 1, 'gamma': 0.2},\n  {'C': 10, 'gamma': 0.1},\n  {'C': 10, 'gamma': 0.2}],\n 'split0_test_score': array([0.75, 0.75, 0.75, 0.75]),\n 'split1_test_score': array([1., 1., 1., 1.]),\n 'split2_test_score': array([1., 1., 1., 1.]),\n 'split3_test_score': array([1., 1., 1., 1.]),\n 'split4_test_score': array([1., 1., 1., 1.]),\n 'mean_test_score': array([0.95, 0.95, 0.95, 0.95]),\n 'std_test_score': array([0.1, 0.1, 0.1, 0.1]),\n 'rank_test_score': array([1, 1, 1, 1], dtype=int32)}"},"metadata":{}}],"id":"18fa6864-5e4e-4d27-af9a-d819e68866a4"}]}