{"metadata":{"language_info":{"name":"python","version":"3.8.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 内容提要\n- Logistic回归: 原理与`sklearn`调用\n- 模型评估的常用指标\n- 交叉验证","metadata":{},"id":"a673f25d-c6f3-4792-857b-ccbfdd06ec3f"},{"cell_type":"markdown","source":"# Logistic回归: 原理与`sklearn`调用\n\n$\\quad$作为监督学习的另一类别, **分类**(classification)问题所需处理的标签$t^{(n)}$是取自类别标签集$\\mathcal{C}$的离散、有限值. 此时, 为了实现连续化(以便对损失函数求梯度), 常用做法是将模型输出以激活函数$y^{(n)} = \\sigma(\\cdot)$做有界化处理, 再根据$y^{(n)}$的取值作出分类决策. 许多模型中, $y^{(n)}$都处于$[0, 1]$范围内, 并被解读为概率.\n\n$\\quad$以二元分类$\\mathcal{C} = {0, 1}$为例, 若将$y^{(n)}$解读为样本分属类别$t = 1$的概率, 则可由Bernoulli分布写出其似然函数, 并按负对数似然(NLL)定义损失函数:\n\n$$\\begin{aligned}\n\\mathcal{L}(\\mathbf{w}; \\mathcal{D}) &:= -\\log{\\left(\\prod_{n=1}^N{(y^{(n)})^{t^{(n)}}}(1 - y^{(n)})^{1 - t^{(n)}}\\right)}\\\\\n&= -\\sum_{n=1}^N{\\left(t^{(n)}\\log{y^{(n)}} + (1 - t^{(n)})\\log{(1 - y^{(n)})}\\right)}.\n\\end{aligned}$$\n\n$\\quad$记类别数目为$c = |\\mathcal{C}|$. 以**独热向量**(one-hot vector)标记样本$n$的真实标签为$\\mathbf{t}^{(n)} \\in \\{0, 1\\}^c$, 将各类别概率整理为(分量和为1的)向量$\\mathbf{y}^{(n)} \\in [0, 1]^c$, 则优化目标事实上等价于**交叉熵**(cross entropy)和**KL散度**(Kullback-Leibler divergence):\n\n$$\\begin{aligned}\n\\mathbf{w}^* &= \\arg{\\min_{\\mathbf{w} \\in \\mathcal{W}}{\\mathcal{L}(\\mathbf{w}; \\mathcal{D})}}\\\\\n&= \\arg{\\min_{\\mathbf{w} \\in \\mathcal{W}}{\\left\\{-\\sum_{n=1}^N{\\mathbf{t}^{(n)} \\cdot \\log{\\mathbf{y}(\\mathbf{x}^{(n)}; \\mathbf{w})}}\\right\\}}} = \\arg{\\min_{\\mathbf{w} \\in \\mathcal{W}}{\\mathrm{CrossEntropy}(\\mathbf{T}, \\mathbf{Y})}}\\\\\n&= \\arg{\\min_{\\mathbf{w} \\in \\mathcal{W}}{\\left\\{\\sum_{n=1}^N{\\mathbf{t}^{(n)} \\cdot \\log{\\mathbf{t}^{(n)}}} - \\sum_{n=1}^N{\\mathbf{t}^{(n)} \\cdot \\log{\\mathbf{y}(\\mathbf{x}^{(n)}; \\mathbf{w})}}\\right\\}}} = \\arg{\\min_{\\mathbf{w} \\in \\mathcal{W}}{D_\\mathrm{KL}(\\mathbf{T} \\| \\mathbf{Y})}}.\n\\end{aligned}$$\n这让我们对Bernoulli NLL有了新的理解: **使预测的类别概率分布尽可能接近真实的(独热)分布**.","metadata":{},"id":"2ef059da-062a-4a5e-acd7-70a95828a075"},{"cell_type":"markdown","source":"## Logistic回归基本原理\n\n$\\quad$**Logistic回归**(Logistic regression)是相当基础但应用广泛的分类模型, 它是**SoftMax回归**(SoftMax regression)在二元分类问题($c = 2$时的特例). 在取交叉熵(也就是Bernoulli NLL)为损失函数的基础上, 我们将假设函数定为(经由Sigmoid函数激活的)线性函数:\n\n$$\ny(\\mathbf{x}; \\mathbf{w}) = \\sigma(\\mathbf{w}^\\mathrm{T}\\mathbf{x}),\\,\\,\\sigma(a) := \\frac{1}{1 + \\mathrm{e}^{-a}}.\n$$\n\n所以, Logistic回归本质也是一种**线性模型**(linear model), 它假定数据标签的**对数似然比**(log odd ratio)关于特征$\\mathbf{x}$是线性的, 即:\n\n$$\n\\log{\\frac{y}{1 - y}} = \\mathbf{w}^\\mathrm{T}\\mathbf{x}.\n$$\n\n这里的隐变量$a = \\mathbf{w}^\\mathrm{T}\\mathbf{x}$被Sigmoid函数处理后, 输出$y$将恒落在$(0, 1)$的界限内, 进而被解释为标签取1的概率. 我们不妨再作出Sigmoid函数的曲线, 加强直观印象.","metadata":{},"id":"694d8244-c407-4678-83e3-78199d0ef4e4"},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport numpy as np\n\ndef sigmoid(a: np.array) -> np.array:\n    return 1 / (1 + np.exp(-a))","metadata":{"trusted":true},"execution_count":1,"outputs":[],"id":"77df4599-7514-4f0a-8a0b-90751a57f700"},{"cell_type":"code","source":"a = np.linspace(-10, 10, num=10000)\nplt.title(\"Sigmoid function\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Sigmoid(x)\")\nplt.plot(a, sigmoid(a))\nplt.axhline(0.5, linestyle=\"--\", color=\"orange\")\nplt.axvline(0, linestyle=\"--\", color=\"orange\")\nplt.show()","metadata":{"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","remote/url":"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17993/d0e069639aa2432e9fdfeb3eb3d0af1d/24VjHyoXRFfe8M-RdLl8Yw.png"},"metadata":{}}],"id":"3687a86a-6c80-449b-8168-1363efe0886b"},{"cell_type":"markdown","source":"## 演示数据集: 乳腺癌肿瘤预测\n\n$\\quad$我们以二元分类数据集[`breast_cancer`](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-wisconsin-diagnostic-dataset)为例进行演示. 这是`sklearn`中的[内置数据集](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer), 收集了细针抽吸活检(fine-needle aspiration, FNA)预测目标是肿瘤的恶性(`malignant`, 标记为1)或良性(`benign`, 标记为0). 我们下面进行数据集的读取与初步探索: 特征维度、样本数目与类别统计. 这里介绍一些有用的新方法:\n- 特征的基本数值分布可用[`df.describe()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html#pandas-dataframe-describe)方法查看;\n- 可用[`df.corr()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html#pandas-dataframe-corr)方法计算特征维度之间的互相关效应(Pearson相关系数), 并以[`seaborn.heatmap()`](https://seaborn.pydata.org/generated/seaborn.heatmap.html#seaborn-heatmap)方法作出直观的相关系数图;\n- 类别标签可由[`df.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html#pandas-dataframe-plot)方法作出直观的图示(例如: 作出分布饼图, 并设置将样本占比输出在饼图上).","metadata":{},"id":"642ccd33-b952-4b25-8b2d-cdfb54423fda"},{"cell_type":"code","source":"from sklearn.datasets import load_breast_cancer\nX, y_true = load_breast_cancer(as_frame=True, return_X_y=True)","metadata":{"trusted":true},"execution_count":3,"outputs":[],"id":"24a6a91d-978d-4b66-b985-f3b631b1fd99"},{"cell_type":"code","source":"# `df.describe()` to inspect feature summary\nX.describe()","metadata":{"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"       mean radius  mean texture  mean perimeter    mean area  \\\ncount   569.000000    569.000000      569.000000   569.000000   \nmean     14.127292     19.289649       91.969033   654.889104   \nstd       3.524049      4.301036       24.298981   351.914129   \nmin       6.981000      9.710000       43.790000   143.500000   \n25%      11.700000     16.170000       75.170000   420.300000   \n50%      13.370000     18.840000       86.240000   551.100000   \n75%      15.780000     21.800000      104.100000   782.700000   \nmax      28.110000     39.280000      188.500000  2501.000000   \n\n       mean smoothness  mean compactness  mean concavity  mean concave points  \\\ncount       569.000000        569.000000      569.000000           569.000000   \nmean          0.096360          0.104341        0.088799             0.048919   \nstd           0.014064          0.052813        0.079720             0.038803   \nmin           0.052630          0.019380        0.000000             0.000000   \n25%           0.086370          0.064920        0.029560             0.020310   \n50%           0.095870          0.092630        0.061540             0.033500   \n75%           0.105300          0.130400        0.130700             0.074000   \nmax           0.163400          0.345400        0.426800             0.201200   \n\n       mean symmetry  mean fractal dimension  ...  worst radius  \\\ncount     569.000000              569.000000  ...    569.000000   \nmean        0.181162                0.062798  ...     16.269190   \nstd         0.027414                0.007060  ...      4.833242   \nmin         0.106000                0.049960  ...      7.930000   \n25%         0.161900                0.057700  ...     13.010000   \n50%         0.179200                0.061540  ...     14.970000   \n75%         0.195700                0.066120  ...     18.790000   \nmax         0.304000                0.097440  ...     36.040000   \n\n       worst texture  worst perimeter   worst area  worst smoothness  \\\ncount     569.000000       569.000000   569.000000        569.000000   \nmean       25.677223       107.261213   880.583128          0.132369   \nstd         6.146258        33.602542   569.356993          0.022832   \nmin        12.020000        50.410000   185.200000          0.071170   \n25%        21.080000        84.110000   515.300000          0.116600   \n50%        25.410000        97.660000   686.500000          0.131300   \n75%        29.720000       125.400000  1084.000000          0.146000   \nmax        49.540000       251.200000  4254.000000          0.222600   \n\n       worst compactness  worst concavity  worst concave points  \\\ncount         569.000000       569.000000            569.000000   \nmean            0.254265         0.272188              0.114606   \nstd             0.157336         0.208624              0.065732   \nmin             0.027290         0.000000              0.000000   \n25%             0.147200         0.114500              0.064930   \n50%             0.211900         0.226700              0.099930   \n75%             0.339100         0.382900              0.161400   \nmax             1.058000         1.252000              0.291000   \n\n       worst symmetry  worst fractal dimension  \ncount      569.000000               569.000000  \nmean         0.290076                 0.083946  \nstd          0.061867                 0.018061  \nmin          0.156500                 0.055040  \n25%          0.250400                 0.071460  \n50%          0.282200                 0.080040  \n75%          0.317900                 0.092080  \nmax          0.663800                 0.207500  \n\n[8 rows x 30 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst radius</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>...</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>14.127292</td>\n      <td>19.289649</td>\n      <td>91.969033</td>\n      <td>654.889104</td>\n      <td>0.096360</td>\n      <td>0.104341</td>\n      <td>0.088799</td>\n      <td>0.048919</td>\n      <td>0.181162</td>\n      <td>0.062798</td>\n      <td>...</td>\n      <td>16.269190</td>\n      <td>25.677223</td>\n      <td>107.261213</td>\n      <td>880.583128</td>\n      <td>0.132369</td>\n      <td>0.254265</td>\n      <td>0.272188</td>\n      <td>0.114606</td>\n      <td>0.290076</td>\n      <td>0.083946</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3.524049</td>\n      <td>4.301036</td>\n      <td>24.298981</td>\n      <td>351.914129</td>\n      <td>0.014064</td>\n      <td>0.052813</td>\n      <td>0.079720</td>\n      <td>0.038803</td>\n      <td>0.027414</td>\n      <td>0.007060</td>\n      <td>...</td>\n      <td>4.833242</td>\n      <td>6.146258</td>\n      <td>33.602542</td>\n      <td>569.356993</td>\n      <td>0.022832</td>\n      <td>0.157336</td>\n      <td>0.208624</td>\n      <td>0.065732</td>\n      <td>0.061867</td>\n      <td>0.018061</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>6.981000</td>\n      <td>9.710000</td>\n      <td>43.790000</td>\n      <td>143.500000</td>\n      <td>0.052630</td>\n      <td>0.019380</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.106000</td>\n      <td>0.049960</td>\n      <td>...</td>\n      <td>7.930000</td>\n      <td>12.020000</td>\n      <td>50.410000</td>\n      <td>185.200000</td>\n      <td>0.071170</td>\n      <td>0.027290</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.156500</td>\n      <td>0.055040</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>11.700000</td>\n      <td>16.170000</td>\n      <td>75.170000</td>\n      <td>420.300000</td>\n      <td>0.086370</td>\n      <td>0.064920</td>\n      <td>0.029560</td>\n      <td>0.020310</td>\n      <td>0.161900</td>\n      <td>0.057700</td>\n      <td>...</td>\n      <td>13.010000</td>\n      <td>21.080000</td>\n      <td>84.110000</td>\n      <td>515.300000</td>\n      <td>0.116600</td>\n      <td>0.147200</td>\n      <td>0.114500</td>\n      <td>0.064930</td>\n      <td>0.250400</td>\n      <td>0.071460</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>13.370000</td>\n      <td>18.840000</td>\n      <td>86.240000</td>\n      <td>551.100000</td>\n      <td>0.095870</td>\n      <td>0.092630</td>\n      <td>0.061540</td>\n      <td>0.033500</td>\n      <td>0.179200</td>\n      <td>0.061540</td>\n      <td>...</td>\n      <td>14.970000</td>\n      <td>25.410000</td>\n      <td>97.660000</td>\n      <td>686.500000</td>\n      <td>0.131300</td>\n      <td>0.211900</td>\n      <td>0.226700</td>\n      <td>0.099930</td>\n      <td>0.282200</td>\n      <td>0.080040</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>15.780000</td>\n      <td>21.800000</td>\n      <td>104.100000</td>\n      <td>782.700000</td>\n      <td>0.105300</td>\n      <td>0.130400</td>\n      <td>0.130700</td>\n      <td>0.074000</td>\n      <td>0.195700</td>\n      <td>0.066120</td>\n      <td>...</td>\n      <td>18.790000</td>\n      <td>29.720000</td>\n      <td>125.400000</td>\n      <td>1084.000000</td>\n      <td>0.146000</td>\n      <td>0.339100</td>\n      <td>0.382900</td>\n      <td>0.161400</td>\n      <td>0.317900</td>\n      <td>0.092080</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>28.110000</td>\n      <td>39.280000</td>\n      <td>188.500000</td>\n      <td>2501.000000</td>\n      <td>0.163400</td>\n      <td>0.345400</td>\n      <td>0.426800</td>\n      <td>0.201200</td>\n      <td>0.304000</td>\n      <td>0.097440</td>\n      <td>...</td>\n      <td>36.040000</td>\n      <td>49.540000</td>\n      <td>251.200000</td>\n      <td>4254.000000</td>\n      <td>0.222600</td>\n      <td>1.058000</td>\n      <td>1.252000</td>\n      <td>0.291000</td>\n      <td>0.663800</td>\n      <td>0.207500</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 30 columns</p>\n</div>"},"metadata":{}}],"id":"be1ce68a-abca-44ff-87c5-1154634870c3"},{"cell_type":"code","source":"# view and plot correlation\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nsns.heatmap(X.corr())\nplt.show()","metadata":{"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","remote/url":"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17993/d0e069639aa2432e9fdfeb3eb3d0af1d/oClRz0ox7fiP5sk71RnBZg.png"},"metadata":{}}],"id":"88af4cca-f7b6-450c-8704-0a2519ab0b0e"},{"cell_type":"code","source":"# pie plot of label distribution\nimport pandas as pd\ny_counts = pd.Series(\n    np.array(y_true.value_counts()), index=[\"benign\", \"malignant\"]\n)\ny_counts.plot(\n    kind=\"pie\",\n    title=\"Distribution of labels\",\n    autopct='%1.1f%%',\n)\nplt.show()","metadata":{"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","remote/url":"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17993/d0e069639aa2432e9fdfeb3eb3d0af1d/-qKvEg7x7hhXR8ckz-dTAw.png"},"metadata":{}}],"id":"156e1f15-d427-41ab-84cd-649fdd00ca3f"},{"cell_type":"markdown","source":"## 模型的训练与求解\n\n$\\quad$以二分类问题为例, 对损失函数\n\n$$\n\\mathcal{L}(\\mathbf{w}; \\mathcal{D}) = - \\sum_{n=1}^N{\\left(t^{(n)}\\log{y^{(n)}} + (1 - t^{(n)})\\log{(1 - y^{(n)})}\\right)}\n$$\n\n求取(关于参数的)梯度, 得到:\n\n$$\n\\nabla_\\mathbf{w}\\mathcal{L} = \\sum_{n=1}^N{y^{(n)}(1 - y^{(n)})\\mathbf{x}^{(n)}},\n$$\n\n随后, 即可利用各种基于梯度的数值优化算法实现模型训练.\n\n$\\quad$在`sklearn`中, Logistic模型以[`sklearn.linear_model.LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn-linear-model-logisticregression)实现调用.\n- 与线性回归模型类似, 模型训练与预测的工作流仍然是: (1) 创建模型对象`clf`; (2) 以训练数据和`clf.fit()`方法实现模型训练; (3) 将特征输入`clf.predict()`(标签预测值)或`clf.predict_proba()`(概率预测值)方法实现新数据的预测.\n- 与线性回归模型不同的点在于, Logistic回归在`sklearn`中的实现是默认加了$\\alpha = 1.0$的$L_2$-范数正则化. 而且正则化强度以$C$表示, 实际等价为$\\alpha$的**倒数**. 所以, `sklearn`中待优化的Logistic回归的目标函数默认为\n\n$$\n\\tilde{\\mathcal{L}} = - C \\sum_{n=1}^N{\\left(t^{(n)}\\log{y^{(n)}} + (1 - t^{(n)})\\log{(1 - y^{(n)})}\\right)} + \\frac{1}{2}\\|\\mathbf{w}\\|_2^2,\n$$\n\n且默认$C=1.0$. 具体的$C$值可通过参数`C`实现调节. 若不希望加正则化, 可设置参数`penalty=\"none\"`.","metadata":{},"id":"d22f5572-d5d4-4163-81ad-b7568f996a19"},{"cell_type":"markdown","source":"$\\quad$首先, 我们将数据集按照**训练:测试=0.75:0.25**的比例进行数据拆分, 并各自进行均值归一化(注意: 利用**训练集**上的均值、方差完成所有归一化). 这步归一化是相当必要的, 因为原始特征各个分量的标度值差别很大.","metadata":{},"id":"957792ef-d02a-40f0-bf7c-2e4dea8b343a"},{"cell_type":"code","source":"# split the dataset with ratio train:test=3:1\ntest_ratio = 0.25\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_true_train, y_true_test = train_test_split(X, y_true, test_size=test_ratio)\nX_train.shape, X_test.shape","metadata":{"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"((426, 30), (143, 30))"},"metadata":{}}],"id":"0988f980-e69e-4c43-b114-4ce48a7fc5d8"},{"cell_type":"code","source":"# rescale the data using `StandardScaler`\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nX_train_scaled.mean(), X_train_scaled.std()","metadata":{"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(7.338939054799157e-17, 1.0)"},"metadata":{}}],"id":"13ee6104-e5b5-4de7-a7a9-9953ee3a383a"},{"cell_type":"markdown","source":"$\\quad$随后, 我们进行模型训练, 并给出训练集、测试集上的真实标签与预测标签. 简化起见, 我们先按默认的超参数设置$C=1.0$进行处理.","metadata":{},"id":"c2851a09-10bd-4883-a41f-aab8bdfbd972"},{"cell_type":"code","source":"# train and test the model\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression().fit(X_train_scaled, y_true_train)\ny_pred_train = clf.predict(X_train_scaled)\ny_pred_test = clf.predict(X_test_scaled)","metadata":{"trusted":true},"execution_count":9,"outputs":[],"id":"837da1e3-877c-4889-9195-7655c147c66f"},{"cell_type":"code","source":"# reset the indices\ny_true_train = y_true_train.reset_index(drop=True)\ny_true_train[:5], y_pred_train[:5]","metadata":{"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(0    0\n 1    1\n 2    1\n 3    0\n 4    1\n Name: target, dtype: int64,\n array([0, 1, 1, 0, 1]))"},"metadata":{}}],"id":"8fd120b4-5515-4a1d-ba4b-f8cbc34a09b9"},{"cell_type":"code","source":"# reset the indices\ny_true_test = y_true_test.reset_index(drop=True)\ny_true_test[:5], y_pred_test[:5]","metadata":{"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(0    1\n 1    1\n 2    1\n 3    0\n 4    1\n Name: target, dtype: int64,\n array([1, 1, 1, 0, 1]))"},"metadata":{}}],"id":"91ca8c10-357d-46b3-bf89-75e592b49b06"},{"cell_type":"markdown","source":"# 模型评估的常用指标\n\n$\\quad$在监督学习中, 一个描述预测能力(predictivity)的指标应能代表了**模型给出的预测结果**与**真实结果**之间的差距. **损失函数**就是一个典型的例子, 它具有连续、光滑的特点, 梯度易于数值计算. 但它本身已经是模型训练采用的优化目标了, 因此不适宜被重复用作模型性能的评估指标(“当一个指标成为目标时, 它就失去了作为指标的评估效用”).\n\n$\\quad$上机实习1已经见到了回归问题的一些常用指标, 例如RMSE、MAE、决定系数$R^2$等等. 本节将介绍分类问题一些常用的评估指标.","metadata":{},"id":"3843ccda-1581-4f12-84cf-69dd7401ef4b"},{"cell_type":"markdown","source":"## 混淆矩阵及其衍生量\n\n$\\quad$最直观的度量分类能力的方式是: 将分类标签的真实值$$t^{(n)} = \\arg{\\max_{1 \\le j \\le c}{\\mathbf{t}^{(n)}}}$$与预测值$$y^{(n)} = \\arg{\\max_{1 \\le j \\le c}{\\mathbf{y}^{(n)}}}$$进行对比, 观察差距. 我们制作一个表格, 每行代表一个真实类别, 每列代表一个预测类别: 在第$i$行第$j$列填入**实际为类别$i$、被模型预测为类别$j$** 的样本的计数. 这个表格给出**混淆矩阵**(confusion matrix) $$\\mathrm{ConfusionMatrix} \\in \\mathbb{R}^{c \\times c},$$它事实上包含了模型给出的分类结果准确程度的所有信息.","metadata":{},"id":"efd60dbc-67fb-4b60-be6e-cd7f1d589c1b"},{"cell_type":"markdown","source":"$\\quad$在`sklearn`中, 混淆矩阵可由[`sklearn.metrics.confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn-metrics-confusion-matrix)方便地给出.","metadata":{},"id":"ea8293b6-a3d3-414b-ae97-24e485bbc873"},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true_test, y_pred_test)\ncm","metadata":{"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"array([[58,  4],\n       [ 1, 80]])"},"metadata":{}}],"id":"eee5aac0-7c97-42f3-a3c0-2b9d19a07d68"},{"cell_type":"markdown","source":"$\\quad$在二分类问题中, 我们记阳性(P)为标签1, 阴性(N)为标签0, 则混淆矩阵给出的样本计数为\n$$\n\\mathrm{ConfusionMatrix} = \\begin{pmatrix}\n\\mathrm{TN} & \\mathrm{FP} \\\\\n\\mathrm{FN} & \\mathrm{TP}\n\\end{pmatrix}.\n$$\n由此可以给出许多衍生量, 例如:\n- **准确率**(accuracy): 模型给出正确预测的能力.\n$$\n\\mathrm{Accuracy} := \\frac{\\mathrm{TP} + \\mathrm{TN}}{\\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN} + \\mathrm{TN}}.\n$$\n- **查准率**(precision)与**查全率**(recall): 针对特定类别(例如阳性P)而言的预测能力.\n  - 查准率: 真阳性占所有预测为阳性的样本的比例(适用场景: FP代价很高, 例如垃圾邮件检测);\n  - 查全率: 真阳性占所有真实为阳性的样本的比例(适用场景: FN代价很高, 例如新冠病情检测).\n  $$\\begin{aligned}\n  \\mathrm{Precision} &:= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}},\\\\\n  \\mathrm{Recall} &:= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}},\\\\\n  \\mathrm{F1} &:= \\frac{2}{\\mathrm{Precision}^{-1} + \\mathrm{Recall}^{-1}}.\n  \\end{aligned}$$\n\n$\\quad$这些量既可以由混淆矩阵出发进行计算, 也可以在[`sklearn.metrics`](https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics)模块中找到对应的函数直接计算.","metadata":{},"id":"7edf7978-aae8-481c-9b84-be4816855cdb"},{"cell_type":"code","source":"tn, fp, fn, tp = cm.reshape(-1)","metadata":{"trusted":true},"execution_count":13,"outputs":[],"id":"6af0c46f-40bb-4005-8217-163cdf049435"},{"cell_type":"code","source":"# accuracy\nfrom sklearn.metrics import accuracy_score\nacc_manual = (tp + tn) / (tp + fp + fn + tn)\nacc_auto = accuracy_score(y_true_test, y_pred_test)\nacc_manual, acc_auto","metadata":{"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(0.965034965034965, 0.965034965034965)"},"metadata":{}}],"id":"d67f0a3e-5ff9-426f-a069-872f0d4ef3d1"},{"cell_type":"code","source":"# precision\nfrom sklearn.metrics import precision_score\nprecision_manual = tp / (tp + fp)\nprecision_auto = precision_score(y_true_test, y_pred_test)\nprecision_manual, precision_auto","metadata":{"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(0.9523809523809523, 0.9523809523809523)"},"metadata":{}}],"id":"646f1b55-705d-4eeb-9125-a79dcf10b8bd"},{"cell_type":"code","source":"# recall\nfrom sklearn.metrics import recall_score\nrecall_manual = tp / (tp + fn)\nrecall_auto = recall_score(y_true_test, y_pred_test)\nrecall_manual, recall_auto","metadata":{"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(0.9876543209876543, 0.9876543209876543)"},"metadata":{}}],"id":"26a27010-35c2-4ecb-9826-9ab33f073604"},{"cell_type":"code","source":"# F1\nfrom sklearn.metrics import f1_score\nf1_manual = 2 / (1 / precision_manual + 1 / recall_manual)\nf1_auto = f1_score(y_true_test, y_pred_test)\nf1_manual, f1_auto","metadata":{"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(0.9696969696969697, 0.9696969696969696)"},"metadata":{}}],"id":"e2443f64-cbc3-452f-b905-815297675894"},{"cell_type":"markdown","source":"## 综合不同的概率阈值\n\n$\\quad$Logistic模型最直接的输出是一个概率向量(可以通过`clf.predict_proba()`方法查看). 在以上的处理中, 我们总是取概率最大的那个类别标签作为预测标签, 这实质上等价于以0.5作为阳性概率的阈值.","metadata":{},"id":"76b5fc9b-5714-404a-a2c5-559d221c92c1"},{"cell_type":"code","source":"X_demo, y_true_demo = X_test_scaled[0].reshape(1, -1), y_true_test.iloc[0]\ny_true_demo = clf.predict(X_demo)\ny_prob_demo = clf.predict_proba(X_demo)\ny_true_demo, y_prob_demo","metadata":{"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(array([1]), array([[2.68008558e-04, 9.99731991e-01]]))"},"metadata":{}}],"id":"55dc1db8-0235-4aeb-9ff9-668c858fdd5b"},{"cell_type":"markdown","source":"$\\quad$但实际问题并非总是适宜以0.5作为概率阈值. 有时, 需要对查准或查全的能力进行严格把关, 阈值需要调整, 对应的阳性、阴性预测结果也就随之变化. 我们以**接收者操作特征曲线**(receiver operating characteristic curve, ROC curve)来直观地表示不同阈值下的预测性能. 这张曲线图中:\n- 横坐标与纵坐标分别为假阳率FPR和真阳率TPR, 计算公式分别为:\n  $$\n  \\mathrm{FPR} := \\frac{\\mathrm{FP}}{\\mathrm{FP} + \\mathrm{TN}}, \\,\\, \\mathrm{TPR} := \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}},\n  $$\n  可见TPR即为阳性样本的查全率, FPR为用1减去阴性样本的查全率.\n- 对每个给定的概率阈值, 分别计算出一组TPR与FPR值、给出一个数据点; 将各个数据点连成曲线, 即给出ROC曲线.\n- 我们还常常计算**曲线下面积**(area under curve, AUC)以度量模型的综合性能. 该值落于区间$[0.5, 1]$内, 且越接近1, 表明性能越好. AUC = 0.5的二元分类器等价于一个随机分类模型(没有学到任何有效信息). 该值常常可以报告为图例, 尤其在涉及多个模型的ROC曲线的比对的场景下. (小小思考题: 如果AUC < 0.5, 说明什么问题? 如何解决?)\n\n$\\quad$可用[`sklearn.metrics.roc_curve`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn-metrics-roc-curve)获得制作ROC曲线所需的一系列FPR、TPR与对应阈值, 并可用[`sklearn.metrics.roc_auc_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score)计算AUC值. 以下将训练集与测试集上的ROC曲线报告在同一张图中, 并以图例区分训练与测试表现.","metadata":{},"id":"3c5dc75d-367b-4f4d-b73a-8bf7e187515c"},{"cell_type":"code","source":"def plot_roc(y_trues, y_scores, labels):\n    from sklearn.metrics import roc_curve, roc_auc_score\n    # figure setup\n    plt.title(\"ROC curve of the Logistic Regression model\")\n    plt.xlabel(\"False positive rate\")\n    plt.ylabel(\"True positive rate\")\n    for y_true, y_score, label in zip(y_trues, y_scores, labels):\n        # metrics preparation\n        fpr, tpr, _ = roc_curve(y_true, y_score)\n        auc = roc_auc_score(y_true, y_score)\n        plt.plot(fpr, tpr, label=f\"{label} AUC = {auc:.2f}\")\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":19,"outputs":[],"id":"295f3cfa-af0c-4ed5-95ea-0d0c578a457b"},{"cell_type":"code","source":"y_prob_train = clf.predict_proba(X_train_scaled)\n# CAUTION: `y_prob` has 2 columns containing probabilities for both labels!\ny_score_train = y_prob_train[:, 1]\ny_prob_test = clf.predict_proba(X_test_scaled)\ny_score_test = y_prob_test[:, 1]","metadata":{"trusted":true},"execution_count":20,"outputs":[],"id":"4e1891e1-8c14-4fa8-a3fa-af5c25880731"},{"cell_type":"code","source":"plot_roc(\n    [y_true_train, y_true_test], [y_score_train, y_score_test],\n    [\"train\", \"test\"]\n)","metadata":{"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","remote/url":"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17993/d0e069639aa2432e9fdfeb3eb3d0af1d/JjtsVLmje4D0Rp-CX0fRXw.png"},"metadata":{}}],"id":"4ed8d1a7-528b-466f-84bb-db42b690199c"},{"cell_type":"markdown","source":"# 交叉验证\n\n$\\quad$在模型**超参数优化**(hyperparameter optimization, HPO)的过程中, 为了不浪费训练数据, 往往采取**交叉验证**(cross validation, CV)的方法. 本次上机实习将演示**k-折**(k-fold)交叉验证: 将训练集$\\mathcal{D}$拆分成若干个数据子集的不交并$\\mathcal{D} = \\mathcal{D}_1 \\cap \\mathcal{D}_2 \\cap \\dots \\cap \\mathcal{D}_k$, 作$k$轮循环. 在每轮循环中留出某一份作验证集, 其余部分作训练集, (独立地)训练一个模型. 将$k$个(初始配置相同、训练集不同的模型的平均性能作为交叉验证的结果.\n```\nD = [D1, ..., Dk]\nfor i = 1, ..., k:\n    model = Model()\n    train, valid = D / D[i], D[i]\n    model.train(train)\n    metric = model.eval(train, valid)\n    metrics.append(metric)\nreport average metrics (or gap)\n```\n\n$\\quad$对于一般的超参数优化, 标准流程是: (1) 设定超参数探索的空间(备选的若干组超参数配置); (2) 对每组超参数配置运行交叉验证, 考察平均性能; (3) 选择性能最优的模型, 在全体训练集$\\mathcal{D}$上完成训练, 用于后续评估与应用. 本次上机实习与作业暂不涉及.\n\n$\\quad$如果认为“模型架构 + 超参数配置”确定了一个模型, 那么超参数优化的过程也可以认为是模型挑选的过程(事实上模型架构相关的参数也可以认为是超参数, 比如神经网络深度、特征个数等). 这也是`sklearn`将数据集分割、交叉验证相关的组件都封装在`model_selection`这一模块的原因. 具体流程示于[下图](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-evaluating-estimator-performance):\n\n![alt image.png](https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17993/77a37c22163c4ca5ab88fd1037dc3035/jToJCYcfqzpYDuNhqCCGcg.png).","metadata":{},"id":"29ce1b07-c4ec-44a1-87be-330cb5f527f7"},{"cell_type":"markdown","source":"$\\quad$以下的代码示出如何使用k-折交叉验证的迭代器[`sklearn.model_selection.KFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn-model-selection-kfold)在数据集`X_train, y_true_train`上实现5折交叉验证. 这种基于迭代器的实现能让我们对数据集的预处理、结果预测等实现更加灵活和模块化的控制.\n\n![alt image.png](https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17993/d0e069639aa2432e9fdfeb3eb3d0af1d/EBIubjxaWllqrqyS1a2SdA.png)","metadata":{},"id":"7b4a57d7-b3c0-4262-8506-02109e2a2dda"},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nn_splits = 5\nkf = KFold(n_splits=n_splits)\naccs = {\"train\": [], \"valid\": []}\nfor train, valid in kf.split(X_train_scaled):\n    # note that the `kf.split` method generates the indices to split\n    X_train_scaled_, X_valid_scaled_, y_true_train_, y_true_valid_ = X_train_scaled[train], X_train_scaled[valid], y_true_train[train], y_true_train[valid]\n    clf = LogisticRegression().fit(X_train_scaled_, y_true_train_)\n    y_pred_train_ = clf.predict(X_train_scaled_)\n    y_pred_valid_ = clf.predict(X_valid_scaled_)\n    accs[\"train\"].append(accuracy_score(y_true_train_, y_pred_train_))\n    accs[\"valid\"].append(accuracy_score(y_true_valid_, y_pred_valid_))\n\naccs","metadata":{"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'train': [0.9911764705882353,\n  0.9882697947214076,\n  0.9882697947214076,\n  0.9824046920821115,\n  0.9853372434017595],\n 'valid': [0.9418604651162791,\n  0.9764705882352941,\n  0.9764705882352941,\n  0.9764705882352941,\n  0.9882352941176471]}"},"metadata":{}}],"id":"0c2d3aa6-f727-4312-b9e0-84a31a2070c8"},{"cell_type":"code","source":"sum(accs[\"train\"]) / len(accs[\"train\"]), sum(accs[\"valid\"]) / len(accs[\"valid\"])","metadata":{"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"(0.9870915991029843, 0.9719015047879618)"},"metadata":{}}],"id":"de2bbcfe-4db2-4e12-8c9e-30b8371b83ab"}]}