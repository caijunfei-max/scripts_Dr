{"metadata":{"language_info":{"name":"python","version":"3.8.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 内容提要\n- 数据处理的一般流程\n- 线性回归: 原理与`sklearn`调用\n- 参数惩罚与模型正则化","metadata":{},"id":"b8d2fcb9-5485-46a5-b148-724f32750ba5"},{"cell_type":"markdown","source":"# 数据处理的一般流程\n\n$\\quad$监督学习的一般场景是, 给定训练数据集$\\mathcal{D} = \\{(\\mathbf{x}^{(n)}, t^{(n)})\\}_{n=1}^N$与模型函数$y(\\mathbf{x};\\mathbf{w})$, 通过损失函数$\\mathcal{L}$的数值优化来确定参数取值(此为模型的**训练**), 即\n$$\n\\mathbf{w}^* = \\arg{\\min_{\\mathbf{w} \\in \\mathcal{W}}{\\mathcal{L}(\\mathbf{w};\\mathcal{D})}}.\n$$\n常见的损失函数包括: 均方误差MSE(用于回归问题), 交叉熵CE(用于分类问题), ...\n\n$\\quad$本节将按照机器学习中一般意义下的数据处理流程, 简要介绍如何运用[`pandas`](https://pandas.pydata.org/)库进行(面板)数据的读取与清洗. 具体包括:\n- 数据的读取与`DataFrame`类;\n- 数据的初步探索;\n- 数据清洗与加工.\n\n$\\quad$本节用于示教的数据集为`sklearn`内置的[鸢尾花(iris)数据集](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html), 特征包括鸢尾花的花萼(sepal)长度与宽度、花瓣(petal)长度与宽度, 标签则是鸢尾花所属的亚种(共3种: `setosa`, `versicolor`, `virginica`).","metadata":{},"id":"4a65b7dd-cd5c-4f8e-b529-0021ddb255a5"},{"cell_type":"markdown","source":"## 数据的读取与`DataFrame`类\n\n$\\quad$由前所述, 容易看出, 监督学习所用的数据集$\\mathcal{D}$往往可以用(特征数据构成的)矩阵与(标签数据构成的)向量来描述.\n\n$\\quad$我们首先将鸢尾花数据集加载为[`pandas.DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)的形式, 这是`pandas`库中用于描述和处理二维数据(常称**面板数据**, panel data)的基本数据结构. 当`load_iris()`函数的参数`as_frame`设置为`True`时,  其返回的数据对象的属性`data`和`target`将为`pandas`库中的`DataFrame`类和[`Series`](https://pandas.pydata.org/docs/reference/api/pandas.Series.html)类(后者用于描述和处理一维序列数据).","metadata":{},"id":"822e6648-c5b3-4bc6-835e-28aa9748aea0"},{"cell_type":"code","source":"from sklearn.datasets import load_iris\ndata = load_iris(as_frame=True)\nX, y_true = data.data, data.target","metadata":{"trusted":true},"execution_count":1,"outputs":[],"id":"72eb95f8-3fb7-4169-a633-a7a6a1a7b011"},{"cell_type":"markdown","source":"## 数据的初步探索\n\n$\\quad$对给定的数据集可先进行初步探索. 例如, 观察`DataFrame`类的下列属性:\n\n- `shape`: 一般地, 每个样本为一行, 每个特征为一列. 若记样本数为$N$, 特征维数为$M$, 则`shape`将输出的形状为$(N, M)$.\n- `columns`: 列. 如果存在列索引(本例是存在的), 则可以观察每一列的名称.\n\n从输出结果可见, 该数据集共有150条样本, 4个特征, 3个类别(且各类别样本数都是50个). `Series`对象内置的[`value_counts`](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html#pandas-series-value-counts)方法可以根据值进行直方图统计.","metadata":{},"id":"d9a804f5-fc76-4020-bca9-038c87825759"},{"cell_type":"code","source":"X.shape, X.columns","metadata":{"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"((150, 4),\n Index(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',\n        'petal width (cm)'],\n       dtype='object'))"},"metadata":{}}],"id":"3fb5fc43-3819-46a6-bbb8-97b2831940a3"},{"cell_type":"code","source":"y_true.shape, y_true.value_counts()","metadata":{"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"((150,),\n 0    50\n 1    50\n 2    50\n Name: target, dtype: int64)"},"metadata":{}}],"id":"2894c2c0-fd16-475d-b415-08219174fce0"},{"cell_type":"markdown","source":"$\\quad$`DataFrame`类中, 索引(indexing)可以有两种方式: ①[按行、列关键字索引](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas-dataframe-loc); ②像`numpy`数组那样, [按行标、列标的整数数值索引](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html#pandas-dataframe-iloc). 若每一列有关键字, 还可以直接像字典索引那样索引特定的一列或几列.","metadata":{},"id":"f3d66541-a320-4a28-afc4-07b887f43d5c"},{"cell_type":"code","source":"# indexing using `loc` method (label based)\nX.loc[:5, \"sepal length (cm)\":\"sepal width (cm)\"]","metadata":{"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   sepal length (cm)  sepal width (cm)\n0                5.1               3.5\n1                4.9               3.0\n2                4.7               3.2\n3                4.6               3.1\n4                5.0               3.6\n5                5.4               3.9","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5.4</td>\n      <td>3.9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"id":"ce5ab023-f015-4d4f-b311-0b802d096b95"},{"cell_type":"code","source":"# indexing using `iloc` method (integer based)\nX.iloc[:5, :2]","metadata":{"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   sepal length (cm)  sepal width (cm)\n0                5.1               3.5\n1                4.9               3.0\n2                4.7               3.2\n3                4.6               3.1\n4                5.0               3.6","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"id":"7ddebeb9-a6ed-4f23-8bf9-360802927765"},{"cell_type":"code","source":"# indexing a column using column keys\nX[\"petal width (cm)\"]","metadata":{"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0      0.2\n1      0.2\n2      0.2\n3      0.2\n4      0.2\n      ... \n145    2.3\n146    1.9\n147    2.0\n148    2.3\n149    1.8\nName: petal width (cm), Length: 150, dtype: float64"},"metadata":{}}],"id":"402a2346-63c4-4074-aaac-0bde2379b9da"},{"cell_type":"code","source":"# multi-column indexing is also supported\nX[[\"sepal length (cm)\", \"sepal width (cm)\"]]","metadata":{"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"     sepal length (cm)  sepal width (cm)\n0                  5.1               3.5\n1                  4.9               3.0\n2                  4.7               3.2\n3                  4.6               3.1\n4                  5.0               3.6\n..                 ...               ...\n145                6.7               3.0\n146                6.3               2.5\n147                6.5               3.0\n148                6.2               3.4\n149                5.9               3.0\n\n[150 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>6.7</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>6.3</td>\n      <td>2.5</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>6.5</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>6.2</td>\n      <td>3.4</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>5.9</td>\n      <td>3.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 2 columns</p>\n</div>"},"metadata":{}}],"id":"380b6297-8ff5-45ce-a84c-06120cafb8be"},{"cell_type":"markdown","source":"## 数据清洗与加工\n\n$\\quad$对原始数据(raw data)的清洗与整理往往是机器学习工作中最为耗时但也最为重要的一步, 这通常包括[缺失值的处理](https://pandas.pydata.org/docs/user_guide/missing_data.html)、数值的归一化、特征的加工等步骤.","metadata":{},"id":"ef3fab75-2d66-4060-9266-8e4320d0187d"},{"cell_type":"markdown","source":"- **缺失值的处理**. 实际问题中, 有的样本可能部分或所有信息是缺失的(在`pandas`中标记为`NaN`, 即not a number).\n  - 最为简单粗暴(但也常常有效)的做法是丢弃这些数据, 这由`DataFrame`对象内置的[`dropna()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html#pandas.DataFrame.dropna)方法可以实现;\n  - 也可以用特定的值来填补缺失部分, 例如用常值0、用同一列特征的均值等, 这由`DataFrame`对象内置的[`fillna()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html#pandas.DataFrame.fillna)方法可以实现.\n\n注意本例没有缺失值, 该环节略去, 只做形式上的演示.","metadata":{},"id":"363abd7a-d4b7-4858-8e2d-4fe6e5624974"},{"cell_type":"code","source":"X.dropna()\nX.shape","metadata":{"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(150, 4)"},"metadata":{}}],"id":"dd3db61e-a9f3-49ab-998a-f0627ad8c66c"},{"cell_type":"markdown","source":"- **数值的归一化**. 不同类型的数据, 其标度(量纲)的不同很可能引起绝对数值上的巨大差异, 而这显然不是我们希望模型学习到的信息. 一种常用的做法是通过归一化等手段对数据数值进行**重标度**(rescaling), 使其数值范围相近. 本例对`sepal length (cm)`分别演示**均值归一化**(mean normalization)\n  $$\n  x_n' := \\frac{x_n - \\mathrm{Mean}_{n=1}^N(x)}{\\mathrm{Std}_{n=1}^N(x)},\\,n=1,\\dots,N\n  $$\n  和**最小-最大归一化**(min-max normalization)\n  $$\n  x_n' = \\frac{x_n - \\min_{n=1}^N{(x)}}{\\max_{n=1}^N{(x)} - \\min_{n=1}^N{(x)}},\\,n=1,\\dots,N,\n  $$\n  并以`value_counts()`方法简要地观察取值分布.\n  - 均值归一化后, 数据以0为中心; 最大-最小归一化后, 数据均处于区间$[0, 1]$内.","metadata":{},"id":"5ae2e510-18a7-465d-9f17-09dad43beed7"},{"cell_type":"code","source":"sep = X[\"sepal length (cm)\"]","metadata":{"trusted":true},"execution_count":9,"outputs":[],"id":"68157647-9a2a-484c-ad3a-a9fab1de2add"},{"cell_type":"code","source":"sep_mean_scaled = (sep - sep.mean()) / sep.std()\nsep_mean_scaled.value_counts(bins=5, sort=False)","metadata":{"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(-1.869, -0.994]    32\n(-0.994, -0.125]    41\n(-0.125, 0.745]     42\n(0.745, 1.614]      24\n(1.614, 2.484]      11\nName: sepal length (cm), dtype: int64"},"metadata":{}}],"id":"4cfaac3e-9c09-4ec5-8d1d-e929cfc6252f"},{"cell_type":"code","source":"sep_minmax_scaled = (sep - sep.min()) / (sep.max() - sep.min())\nsep_minmax_scaled.value_counts(bins=5, sort=False)","metadata":{"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(-0.002, 0.2]    32\n(0.2, 0.4]       41\n(0.4, 0.6]       42\n(0.6, 0.8]       24\n(0.8, 1.0]       11\nName: sepal length (cm), dtype: int64"},"metadata":{}}],"id":"01196009-6b33-4e69-bafb-0d97a3caf817"},{"cell_type":"markdown","source":"- **特征的加工**. 原始特征(或标签)有时并非是模型学习的最佳选择, 我们完全可以自主地从原始数据出发, 提取和加工新的特征. 例如, 将花瓣的长度、宽度相乘, 给出其外切正方形的面积, 以期对花瓣的特征进行降维. 两个序列数据(或一维向量)的逐项乘积(在矩阵代数中称为**Hadamard乘积**)可以直接用运算符`*`表示, 这是`numpy`的数组运算规范.","metadata":{},"id":"e8835cb5-b310-4c50-a3e6-47ee8d67e42f"},{"cell_type":"code","source":"petal_area = X[\"petal length (cm)\"] * X[\"petal width (cm)\"]\npetal_area.value_counts(bins=5, sort=False)","metadata":{"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(0.0932, 3.262]    50\n(3.262, 6.414]     32\n(6.414, 9.566]     29\n(9.566, 12.718]    25\n(12.718, 15.87]    14\ndtype: int64"},"metadata":{}}],"id":"3476eabf-297a-4350-aca5-2c0fc99fbd95"},{"cell_type":"markdown","source":"# 线性回归: 原理与`sklearn`调用\n$\\quad$作为回归(regression)模型中最简单的一类, **线性回归**(linear regression)模型将拟合一个线性函数\n\n$$\ny(\\mathbf{x};\\mathbf{w}) = \\mathbf{w}^\\mathrm{T}\\mathbf{x} + b = \\sum_{j=1}^M{w_jx_j} + b,\n$$\n\n并通常以均方误差(MSE)作为损失函数:\n\n$$\\begin{aligned}\n\\mathcal{L}(\\mathbf{w};\\mathcal{D})\n&= \\frac{1}{N}\\sum_{n=1}^N{(t^{(n)} - y(\\mathbf{x}^{(n)};\\mathbf{w}))^2}\\\\\n&= \\frac{1}{N}\\sum_{n=1}^N{(t^{(n)} - \\mathbf{w}^\\mathrm{T}\\mathbf{x} - b)^2}.\n\\end{aligned}$$\n\n$\\quad$线性回归模型是可以解析求解的. 将训练数据$\\mathcal{D}$组合成特征矩阵$\\mathbf{X} \\in \\mathbb{R}^{N \\times (M+1)}$(包含截距列$\\mathbf{1}_M$)与标签向量$\\mathbf{t} \\in \\mathbb{R}^N$, 则解析解写为\n\n$$\n\\mathbf{w}^* = (\\mathbf{X}^\\mathrm{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathrm{T}\\mathbf{t}.\n$$\n\n不过, 更普适的求解方式是基于梯度的一些数值优化算法, 例如**随机梯度下降**(stochastic gradient descent, SGD).","metadata":{},"id":"3fd4c4a3-0320-4063-83e9-edae9eaaa8f5"},{"cell_type":"markdown","source":"## 人工合成的数据集\n\n$\\quad$本节演示的数据集是一个**人工合成的数据集**(synthetic dataset). 这也许不具有实践价值, 但在解释机器学习模型的工作机制、对模型性能进行初步测试等场景中往往具有更简洁、更深刻的效果. 我们构造三个彼此独立的标准正态变量$x_1, x_2, x_3 \\sim \\mathcal{N}(0, 1)$, 并将它们的线性组合加上Gaussian噪声$\\epsilon \\sim \\mathcal{N}(0, 0.1)$, 作为标签:\n\n$$\ny = x_1 + 2x_2 - x_3 + \\epsilon.\n$$","metadata":{},"id":"53c287fb-6234-472a-a380-537d0272a7f4"},{"cell_type":"code","source":"from typing import List, Tuple\nimport numpy as np\nfrom numpy.random import normal\n\ndef gen_synthetic_dataset(\n    n_samples: int=100, coefs: List[float]=[1.0, 2.0, -1.0],\n    noise: Tuple[float, float]=(0, 0.1)\n) -> Tuple[np.array, np.array]:\n    \"\"\"\n    Generates a synthetic dataset for linear regression.\n    \n    Args\n    ----\n    n_samples: int = 100\n        The number of samples in a dataset. Default `100`.\n    coefs: List[float] = [1.0, 2.0, -1.0]\n        The coefficients of features. Default `[1.0, 2.0, -1.0]`.\n    noise: Tuple[float, float] = (0, 0.1)\n        The distribution parameter of Gaussian noise, formatted as `(loc, scale)`.\n        Default `(0, 0.1)`.\n    \n    Returns\n    -------\n    X: np.array\n        The feature matrix in shape `(n_samples, n_features)`.\n    y_true: np.array\n        The label vector in shape `(n_samples,)`.\n    \"\"\"\n    loc, scale = noise\n    X, y_true = [], []\n    for _ in range(n_samples):\n        x = normal(scale=10 * scale, size=len(coefs))\n        eps = normal(loc=loc, scale=scale)\n        X.append(x)\n        y_true.append(x.dot(np.array(coefs)) + eps)\n    return np.array(X), np.array(y_true)","metadata":{"trusted":true},"execution_count":13,"outputs":[],"id":"cbac7ffd-a558-401b-9bbe-58f8a8653b24"},{"cell_type":"code","source":"X, y_true = gen_synthetic_dataset(n_samples=5)\nX, y_true","metadata":{"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(array([[-0.32377084,  0.48590415, -0.01121846],\n        [ 0.38083368,  0.64312977, -0.05127355],\n        [ 0.92784811,  1.14124349,  0.00520954],\n        [-0.50613695,  0.70224343, -0.13578083],\n        [-0.79012599, -1.12715899,  0.06703884]]),\n array([ 0.72166231,  1.58143196,  3.11874869,  1.06812295, -3.11936939]))"},"metadata":{}}],"id":"78255433-ed26-4aa9-a5c2-afbe3ae814fc"},{"cell_type":"markdown","source":"## `sklearn`库中的模型调用接口\n\n$\\quad$在[`sklearn`](https://scikit-learn.org/)中, 所有监督学习模型的调用框架都是统一的, 我们先创建一个模型对象, 并传入训练数据以`fit`方法进行训练, 随后以`predict`方法对输入的新数据(特征)进行预测. 下面以线性回归模型[`sklearn.linear_model.LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)为例, 我们用一个含100个样本的数据集作为训练数据、再用一个新的含5个样本的新数据集作为测试数据.\n\n$\\quad$由于人工数据集本身就是从线性组合给出的, 所以, 可以从测试结果中看到, 模型很容易地捕捉到了人工数据集的内禀规律.","metadata":{},"id":"4ec813ff-52d6-4670-b007-99fed8a3647c"},{"cell_type":"code","source":"# instantiate a model\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()","metadata":{"trusted":true},"execution_count":15,"outputs":[],"id":"8738782a-b1fc-4578-adc9-cbbef8ece170"},{"cell_type":"code","source":"# train a model using the synthetic training set\nX_train, y_true_train = gen_synthetic_dataset()\nlr.fit(X_train, y_true_train)","metadata":{"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"LinearRegression()"},"metadata":{}}],"id":"4432592b-d888-4f93-891b-0ee9e0a10541"},{"cell_type":"code","source":"# make new predictions\nX_test, y_true_test = gen_synthetic_dataset(n_samples=5)\ny_pred_test = lr.predict(X_test)\ny_true_test, y_pred_test","metadata":{"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(array([ 0.45211497, -0.01540541,  3.52035423,  0.21328979,  4.20037346]),\n array([ 0.34990727, -0.0747876 ,  3.57556335,  0.09496243,  4.21432821]))"},"metadata":{}}],"id":"c3c53af4-6843-4841-8942-4995271cdd65"},{"cell_type":"markdown","source":"## 将运行结果可视化\n\n我们很容易用[`matplotlib`](https://matplotlib.org/)库将模型在训练集上的表现进行可视化. 以下的代码将生成一张描述模型在一个50样本测试集上预测表现的散点图, 包括这三个方面的信息:\n- 散点, 描述各样本. 横坐标为真实标签, 纵坐标为预测标签.\n- 直线$y=x$(以虚线示出). 显然, 散点越靠近这条直线, 表明预测标签离真实标签就越近.\n- **决定系数**(coefficient of determination)\n  $$\n  R^2(\\mathbf{t}, \\mathbf{y}) := 1 - \\frac{\\sum_{n=1}^N{(t^{(n)} - y^{(n)})^2}}{\\sum_{n=1}^N{(t^{(n)} - \\bar{t})^2}}\n  $$\n  可由[sklearn.metrics.r2_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn-metrics-r2-score)方便地给出计算, 结果作为图片标题的一个部分.","metadata":{},"id":"1d2fe636-7d83-4539-a5c2-b3fc6122fc19"},{"cell_type":"code","source":"def plot_prediction(y_true: np.array, y_pred: np.array):\n    from matplotlib import pyplot as plt\n    from sklearn.metrics import r2_score\n    r2 = r2_score(y_true, y_pred)\n    # text annotation setup\n    plt.title(r\"True values vs predicted values ($R^2$ = \" + f\"{r2:.4f}\" + \")\")\n    plt.xlabel(\"True values\")\n    plt.ylabel(\"Predicted values\")\n    # plot the scatter and line\n    plt.scatter(y_true, y_pred, c=\"red\", marker=\"o\")\n    plt.plot(y_true, y_true, \"b--\")\n    # show the plot!\n    plt.show()","metadata":{"trusted":true},"execution_count":18,"outputs":[],"id":"6bacff79-f668-402e-9503-3da7c12cdf28"},{"cell_type":"code","source":"X_test, y_true_test = gen_synthetic_dataset(n_samples=50)\ny_pred_test = lr.predict(X_test)\nplot_prediction(y_true_test, y_pred_test)","metadata":{"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","remote/url":"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17993/263dbadb804840da894877740bb2aa01/wXwj0SZ9T8hHUxUCOoiSdA.png"},"metadata":{}}],"id":"cfb73f51-c07c-4de3-91fb-c948d92b59e9"},{"cell_type":"markdown","source":"# 参数惩罚与模型正则化\n\n$\\quad$为了缓解**过拟合**(overfitting)现象, 一种常用的策略是模型**正则化**(regularization): 在损失函数中添加参数的$L_2$-范数惩罚项，\n\n$$\n\\tilde{\\mathcal{L}} := \\mathcal{L} + \\alpha\\|\\mathbf{w}\\|^2,\n$$\n\n其中, $\\alpha$称为**正则化强度**(regularization strength), 控制着$\\mathbf{w}\\to\\mathbf{0}$的倾向性. 理论课程已经讲过, 若参数先验和数据似然都取Gaussian分布的形式, 则$L_2$-正则化等价于**收缩参数的先验分布窗口**.\n\n$\\quad$带有$L_2$-正则化项的线性回归也称为**岭回归**(ridge regression), 在`sklearn`中以[`sklearn.linear_model.Ridge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn-linear-model-ridge)实现. 其中, 参数`alpha`即为正则化强度. 以下的代码构建一个$\\alpha=10.0$的岭回归模型`lr_reg`, 并和`lr`模型以相同的训练集进行训练、在相同测试集上进行评估. 可见, 正则化在一定程度上会带来决定系数(准确度)的牺牲.","metadata":{},"id":"09e58ba0-18d4-4ae7-beba-185987335e7a"},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\nlr_reg = Ridge(alpha=10.0)\nlr_reg.fit(X_train, y_true_train)\ny_reg_pred_test = lr_reg.predict(X_test)\nplot_prediction(y_true_test, y_reg_pred_test)","metadata":{"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","remote/url":"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17993/263dbadb804840da894877740bb2aa01/0DBWMD-eTZYSyICV3E0bqA.png"},"metadata":{}}],"id":"264e197b-b12b-4918-b064-0b398d78b76c"},{"cell_type":"markdown","source":"$\\quad$模型正则化是一种较精细地调控模型复杂度的手法. 正则化强度越大, 模型容量、复杂度越低(预测能力会打一些折扣), 但相应地会提升泛化能力和稳定性. 以下取$\\alpha = 10^{-3}, 10^{-2}, 10^{-1}, 10^0, 10^1, 10^2$分别构建并训练模型, 给出训练集与测试集上的**根均方误差**(RMSE), 以及二者的差值(称为**泛化间隔**, generalization gap), 以作比较. 为了突出过拟合现象, 我们重新构造只含10个样本的训练集.","metadata":{},"id":"f82927da-eb27-405a-ad49-77dd54f5e7fe"},{"cell_type":"code","source":"X_train, y_true_train = gen_synthetic_dataset(n_samples=10)","metadata":{"trusted":true},"execution_count":21,"outputs":[],"id":"3e8e6144-a32a-4e23-abae-ea1fc979f697"},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error as MSE\n\nrmse_train, rmse_test, rmse_gap = [], [], []\nalphas = (1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2)\nfor alpha in alphas:\n    lr_reg = Ridge(alpha=alpha).fit(X_train, y_true_train)\n    y_pred_train = lr_reg.predict(X_train)\n    y_pred_test = lr_reg.predict(X_test)\n    rmse_train.append(MSE(y_true_train, y_pred_train, squared=False))\n    rmse_test.append(MSE(y_true_test, y_pred_test, squared=False))\n    rmse_gap.append(rmse_test[-1] - rmse_train[-1])","metadata":{"trusted":true},"execution_count":22,"outputs":[],"id":"9e1142e0-d08c-4bda-b04e-4f4c5a5e2864"},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nplt.title(\"Training and test error under different regularization strengths\")\nplt.xlabel(r\"$\\alpha$\")\nplt.xscale(\"log\")\nplt.ylabel(\"RMSE\")\nplt.plot(alphas, rmse_train, marker=\"o\", label=\"train\")\nplt.plot(alphas, rmse_test, marker=\"o\", label=\"test\")\nplt.plot(alphas, rmse_gap, marker=\"o\", label=\"generalization gap\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","remote/url":"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17993/263dbadb804840da894877740bb2aa01/UfDROGhexB_BYzrgqyGHjw.png"},"metadata":{}}],"id":"a06756ad-ecda-480b-9e4c-e940b4d6cd22"},{"cell_type":"markdown","source":"$\\quad$正则化强度$\\alpha$是模型的超参数. 以上的过程是一个简单的**超参数搜索**(hyperparameter searching)的例子. 泛化间隔最小的模型是$\\alpha=1.0$, 它于是被认为是本例的最优超参数.","metadata":{},"id":"03388768-8392-466c-8fc9-c1eb381ae0f7"}]}